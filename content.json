{"meta":{"title":"Crazy Cloud Ideas","subtitle":"","description":"","author":"Sameera Perera","url":"https://sameeraman.github.io","root":"/blog/"},"pages":[{"title":"About","date":"2020-07-07T10:46:05.000Z","updated":"2022-01-29T06:18:40.816Z","comments":false,"path":"about/index.html","permalink":"https://sameeraman.github.io/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2020-07-07T10:44:47.000Z","updated":"2022-01-29T06:18:40.816Z","comments":false,"path":"categories/index.html","permalink":"https://sameeraman.github.io/categories/index.html","excerpt":"","text":""},{"title":"My Pet Projects","date":"2020-07-07T11:34:14.000Z","updated":"2022-01-29T06:18:40.816Z","comments":false,"path":"petprojects/index.html","permalink":"https://sameeraman.github.io/petprojects/index.html","excerpt":"","text":"This will the pet projects page"},{"title":"Tags","date":"2020-07-04T10:45:11.000Z","updated":"2022-01-29T06:18:40.816Z","comments":false,"path":"tags/index.html","permalink":"https://sameeraman.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"azuresiterecoveryplan","slug":"azuresiterecoveryplan","date":"2022-01-29T06:18:40.800Z","updated":"2022-01-29T06:18:40.800Z","comments":true,"path":"2022/01/29/azuresiterecoveryplan/","link":"","permalink":"https://sameeraman.github.io/2022/01/29/azuresiterecoveryplan/","excerpt":"","text":"Azure Site Recovery Plan Example ExplainedRecently I was working on a demo for a customer on Azure Site Recovery Plans and I thought I share my setup in here so that it would be beneficial for someone else as well. IntroductionIn summary, I have an environment that has two virtual networks, one in West US 2 and one in East US. Both of the networks are connected to the on-premises via a central hub network. I have a two tier application that runs in the West US 2 region. The sample application is a Todo Application that has a DotNet core 6 web application and a SQL backend (https://github.com/sameeraman/DotNetCoreSQLDB). So the overall setup looks as below. ExperienceWhen you access the application in the Primary (West US 2) region, you will see the application as below. I have enabled ASR replication on the the application server and the SQL Server. Then Create a Recovery plan under the Azure Site recovery. This process is straight forward. In Production environments, you would normally have SQL HA/DR setup for the database to failover. Eg. You would configure SQL Always-On Replication between the two regions. You can have a asynchronously replicating database on the East US region. Then you will failover the database on the SQL Level rather than the VM level that is used in this sample. I want to show you how to tune the failover sequence and also to customize the failover with custom script activities. You can add custom scripts via Azure automation account runbooks to the ASR Recovery plans. This make it possible to literally do anything you want as part of your recovery plan. Some of the common uses of the custom scripts are below: Add a VM Behind a load balancer after the failover Create a Public IP for the failed over VM. Update NSG rules with Failed over VM IP addresses. Failover SQL Always on Availability Group to the DR with the Servers. Update DNS records post failover In the above example, I’ll be using a custom script to update the SQL connection string, SQL Server IP address with the new IP address. I use the Invoke-AzVMRunCommand Powershell command to run a script on the server to update the connection string. Have a look at the Runbook PowerShell script here - [UpdateSQLIP.ps1] (https://github.com/sameeraman/Scripts/blob/master/ASRRunbookSample/UpdateSQLIP.ps1) I also have another script that add a 1 minute delay for the failover between the two servers. It’s actually not required, but I have added it to showcase another use of scripts. That script can be found in the link here - Delay1minute.ps1. So putting them all together in the Recovery plan, it looks as below. The Runbooks in the automation Account looks as below: When you failover using this Recovery Plan, you will see the application comes up successfully on the East US region and the application accessible using the new IP. You will see the Connection string has been updated with the new IP address after the failover. As you can see the application has been successfully failed over to the secondary region. The orchestration was fully automated end-to-end. Using the same recovery plan, you can also fail back to the primary region again. But I will not cover that in this blog. ConclusionAs you can see, Azure Site Recovery Plans let’s you fully automate the application infrastructure failover and reduce the RTO in your application. It allows you to add pre and post actions for each group you failover. Azure Automation Runbook scripts make it possible to execute anything you like as pre and post tasks.","categories":[],"tags":[]},{"title":"AKS Persistent Storage – Azure Files Dynamic Provisioning","slug":"aksstorageazfilesdynamic","date":"2021-11-07T11:00:00.000Z","updated":"2022-01-29T06:18:40.796Z","comments":true,"path":"2021/11/07/aksstorageazfilesdynamic/","link":"","permalink":"https://sameeraman.github.io/2021/11/07/aksstorageazfilesdynamic/","excerpt":"As you create and manage AKS clusters, you will soon enough understand that your application needs persistent storage to store application data. There are many different options available in AKS with Azure Native services. In this blog post we will be closing looking at using azure files as the persistent storage with the dynamic provisioning capabilities.","text":"As you create and manage AKS clusters, you will soon enough understand that your application needs persistent storage to store application data. There are many different options available in AKS with Azure Native services. In this blog post we will be closing looking at using azure files as the persistent storage with the dynamic provisioning capabilities. When you create a AKS cluster you will notice that you get a few storage classes already added to it. Azure files premium and azure files standard storage classes comes by default in the AKS clusters. You can leverage these storage classes to dynamically create persistent storage for your application. Let’s look at an example and see how the AKS cluster behaves at the backend. The commands and the deployment information are in this github repo. I assume that you have got a Kubernetes cluster already running. We will be creating a PVC with Azure files premium and a deployment with nginx to use it. First use the dynamic_provisioning.yaml file to create the pvc and the deployment. Then remote into the pod that is running. Navigate to the mapped share and create a file as below. Scale the deployment to 5 instances as below. Once all the pods are running, remote into a different pod and run the following commands. Then go to the Azure Portal, browse to the azure files storage account, and check the content. As you can see, the storage is available across multiple pods, and the pods are able to read write to the storage at the same time. The storage content is also available via the Azure Portal.","categories":[{"name":"AKS","slug":"aks","permalink":"https://sameeraman.github.io/categories/aks/"}],"tags":[{"name":"AKS","slug":"aks","permalink":"https://sameeraman.github.io/tags/aks/"},{"name":"Kuberenetes","slug":"kuberenetes","permalink":"https://sameeraman.github.io/tags/kuberenetes/"},{"name":"Storage","slug":"storage","permalink":"https://sameeraman.github.io/tags/storage/"},{"name":"Files","slug":"files","permalink":"https://sameeraman.github.io/tags/files/"}]},{"title":"Server to Server Data copy using AzCopy","slug":"azcopydatatransfer","date":"2021-09-14T11:00:00.000Z","updated":"2022-01-29T06:18:40.800Z","comments":true,"path":"2021/09/14/azcopydatatransfer/","link":"","permalink":"https://sameeraman.github.io/2021/09/14/azcopydatatransfer/","excerpt":"If you are migrating your workloads to the cloud, you most probably have come across in scenarios where you need to copy large amount of data cross the network into the cloud. In this post, I’m going to be talking about my experience in one of those scenarios and my observations. The observations and learning are interesting and worth sharing. Therefore, continue reading this article to the end.","text":"If you are migrating your workloads to the cloud, you most probably have come across in scenarios where you need to copy large amount of data cross the network into the cloud. In this post, I’m going to be talking about my experience in one of those scenarios and my observations. The observations and learning are interesting and worth sharing. Therefore, continue reading this article to the end. Problem StatementRecently, I was involved in a Server-to-Server data migration over the Azure ExpressRoute. This scenario had a high latency connection from the on-promises to the cloud; it was around 60ms route trip time. I initially used Robocopy tool to migrate the files and wasn’t experiencing a great bandwidth consumption during the migration. I was able to achieve maximum 280Mbps even though I used multithread data copy using the robocopy option. I had plenty of capacity available in the ExpressRoute, however, robocopy was not pushing the data transfer speeds beyond this limit.Following is an example small copy result. As you can see, it was only achieving 265Mbps. InvestigationWhile reading about the Robocopy, I learned, even if we use multi-threaded flag, with high latency data copy it doesn’t achieve a high speed because it uses SMB for the data transfer. In other words, Robocopy is not the best tool to copy data across high latency connections. The reason for that is SMB protocol is not designed to copy with high latency connections. There is a single TCP connection maximum bandwidth limit to be aware here has well. In Azure, there are some measured single session TCP maximum bandwidth limits documented here. This will give you a guide around the limits. Moving on from Robocopy, I used FTP as another method for the copy job. I used FileZilla tool for this. This required an FTP server to be configured on the source side; and the data to be pulled from an FTP client at the target side. Note that this also required additional ports (port 21 by default) opened between the source server and the destination.I managed to achieve slightly higher data transfer rates using this method, however, still it wasn’t utilizing the full ExpressRoute bandwidth. This resulted in me looking for other options. SolutionI initially looked at using azcopy tool for this job. However, I didn’t go with this path because it can only copy files to an Azure Storage account, e.g. Blob store. Because of the low maximum bandwidth achieved from the robocopy and FTP, I was compelled to investigate this method again. My requirement was to copy files from the source server to the destination server, therefore with azcopy, I had to (1) first upload the files to a staging storage account, then (2) download the files the destination server. Even though this is a two-step approach, this gave me faster throughputs during the copy. This resulted in a significantly quicker end to end completion time compared to the robocopy. The key point to mention here is, azcopy is designed for high latency copy jobs. Therefore, it pushes the throughputs to the maximum available in the link. With the usage of the Azure Private endpoints, the route of the file transfer traffic, still stays the same. It copies the files over the ExpressRoute to the same virtual network. Therefore, from a security perspective, it’s not a huge compromise. What is significant to note in here is the download speed from the storage account to the target server. Because they are in the same region, it managed to achieve throughputs as high as 10Gbps. Therefore, time taken in the step 2 is neglectable when compared to the step 1. Following is an example upload speed achieved. Following is an example download speed achieved. ConclusionThe conclusion here is, direct copy using traditional methods (like robocopy) in high latency scenarios, especially migrating data to the cloud, is not always the best method. There are other tools specifically designed for high latency scenarios (like azcopy). Even though they can’t do server-to-server direct copy, they can perform better in a two-step approach to migrate the files to the destination/cloud.","categories":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/categories/networking/"}],"tags":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/tags/networking/"},{"name":"ExpressRoute","slug":"expressroute","permalink":"https://sameeraman.github.io/tags/expressroute/"},{"name":"PrivateLink","slug":"privatelink","permalink":"https://sameeraman.github.io/tags/privatelink/"},{"name":"AzCopy","slug":"azcopy","permalink":"https://sameeraman.github.io/tags/azcopy/"}]},{"title":"Azure Management Groups Activity logs to Azure Monitor","slug":"mgauditlogs","date":"2021-06-20T18:34:48.000Z","updated":"2022-01-29T06:18:40.808Z","comments":true,"path":"2021/06/20/mgauditlogs/","link":"","permalink":"https://sameeraman.github.io/2021/06/20/mgauditlogs/","excerpt":"This blog post shows you how to forward your Azure Management Group Activity logs to Azure Monitor or any SIEM product that you have in your environment. There is a known limitation that this cannot be configured in the Azure Portal. It needs to be enabled using the backed API and this post will provide the necessary details for it. This is important for some Azure Customers as Management Groups defines the organizations top level governances, including Azure Policy and Access Management.","text":"This blog post shows you how to forward your Azure Management Group Activity logs to Azure Monitor or any SIEM product that you have in your environment. There is a known limitation that this cannot be configured in the Azure Portal. It needs to be enabled using the backed API and this post will provide the necessary details for it. This is important for some Azure Customers as Management Groups defines the organizations top level governances, including Azure Policy and Access Management. Problem StatementAs of today (20/06/2021), Azure Portal does not allow you to configure Diagnostic settings at the management group level. If you go to Diagnostic settings at the management groups, it takes you to a page where it asks again for a resource to configure diagnostic settings on – see below animation. Not that you don’t get a place to configure diagnostic setting for management groups. This is a known limitation. You can see the activity logs on the Azure Portal for Management groups if you go to the Activity Logs pane on a management group; however, you cannot forward them to Azure Monitor nor any SIEM project. This is a problem for some customers, as they want to use Azure Monitor to monitor the entire cloud landscape rather than browsing through different locations. Furthermore, some customers want to query a single log analytics workspace for platform level analytics. The only way to do that is to forward all the logs to the Azure Monitor. Without wasting anymore time, lets look how it can be done. SolutionThe good news is - there is a way that this can be achieved. The diagnostic settings at the management group level can only be enabled and configured via the APIs at this time. The API is available in the following location. https://docs.microsoft.com/en-us/rest/api/monitor/managementgroupdiagnosticsettings/createorupdate With this API you can configure diagnostic settings on the management groups. Diagnostic settings allow you to configure forwarding rule for activity logs to 3 locations, consistent with any other resource type in Azure. They are as below. Log analytics workspace Storage account Event Hubs. Before you call this API, you will need to have a valid bearer token acquired through the Azure AD. If you are unfamiliar with the process on calling the Azure API’s. This API documentation is a good place to start. There are certain pre-requisites items such as creating a service principal and assigning permissions that needs to occur, this is detailed in the documentation along with a youtube video.I used Postman as the tool to call the above API. Following are some of the details of the API calls I made to set this diagnostic settings Get the Bearer TokenI used the following API to get the Bearer token from the Azure AD. 1https:&#x2F;&#x2F;login.microsoftonline.com&#x2F;&lt;tenant id&gt;&#x2F;oauth2&#x2F;token The request and the response screenshot in Postman is below. List the Existing Diagnostic Settings on a Given Management Group.I used the following API to list the existing diagnostic configuration on the management group. 1https:&#x2F;&#x2F;management.azure.com&#x2F;providers&#x2F;microsoft.management&#x2F;managementGroups&#x2F;wtt-ml-sandboxes&#x2F;providers&#x2F;microsoft.insights&#x2F;diagnosticSettings?api-version&#x3D;2020-01-01-preview The request and response of the GET call in Postman looks like below. Configure a Diagnostic Setting to send logs to a Log Analytics workspace.I used the following API to update the diagnostic configuration on the management group wtt-ml-sandboxes. 1https:&#x2F;&#x2F;management.azure.com&#x2F;providers&#x2F;microsoft.management&#x2F;managementGroups&#x2F;wtt-ml-sandboxes&#x2F;providers&#x2F;microsoft.insights&#x2F;diagnosticSettings&#x2F;testdiag?api-version&#x3D;2020-01-01-preview The request and response of the PUT call in Postman looks like below. This is all you need to configure the diagnostic settings on the management group. The configuration takes 5-10 minutes for logs to appear in the log analytics workspace. If you go to the Log analytics workspace, you will see the data getting logged in the same AzureActivity Logs table. The log format is as below. You can use this table and the data in any Log Analytics query that you want to formulate.","categories":[{"name":"Governance","slug":"governance","permalink":"https://sameeraman.github.io/categories/governance/"}],"tags":[{"name":"Governance","slug":"governance","permalink":"https://sameeraman.github.io/tags/governance/"}]},{"title":"Azure Virtual WAN N region Bicep Template","slug":"vwanntemplate","date":"2021-04-10T09:34:48.000Z","updated":"2022-01-29T06:18:40.816Z","comments":true,"path":"2021/04/10/vwanntemplate/","link":"","permalink":"https://sameeraman.github.io/2021/04/10/vwanntemplate/","excerpt":"This blog post is about a Bicep Template that I authored to create a lab environment for customer scenarios and use cases. It creates a VWAN with N number of VWAN hubs in N regions. It also creates spoke VNet in each region including a VM. The template can be found in the following location with further details on how to use it.","text":"This blog post is about a Bicep Template that I authored to create a lab environment for customer scenarios and use cases. It creates a VWAN with N number of VWAN hubs in N regions. It also creates spoke VNet in each region including a VM. The template can be found in the following location with further details on how to use it. https://github.com/sameeraman/n_vwan_bicep_template Some of the example implementations that you can do using the template are below. Azure VWAN 2 Region SetupYou can provision an Azure VWAN 2 region model including 2 VWAN Hubs, a VNET and VM in each region. The final solution after deployment will look like below. Azure VWAN 3 Region SetupYou can also provision a setup with 3 VWAN hubs like above. The final solution after deployment will like below.","categories":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/categories/networking/"}],"tags":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/tags/networking/"},{"name":"VWAN","slug":"vwan","permalink":"https://sameeraman.github.io/tags/vwan/"},{"name":"Bicep","slug":"bicep","permalink":"https://sameeraman.github.io/tags/bicep/"}]},{"title":"Azure Global Reach in Action","slug":"globalreach","date":"2020-11-04T11:24:44.000Z","updated":"2022-01-29T06:18:40.804Z","comments":true,"path":"2020/11/04/globalreach/","link":"","permalink":"https://sameeraman.github.io/2020/11/04/globalreach/","excerpt":"Recently, I was lucky to get two ExpressRoutes in a Microsoft Lab environment so that I could test the Azure Global Reach in Action. ExpressRoutes are not something you can test typically like the other resources in Azure, because it requires, customer router configuration, last mile provider configuration. Therefore, I thought I would share my experience with the rest of the world. This blog post is around the experience, a bit of a glimpse, to what happens when you enable Azure Global Reach.","text":"Recently, I was lucky to get two ExpressRoutes in a Microsoft Lab environment so that I could test the Azure Global Reach in Action. ExpressRoutes are not something you can test typically like the other resources in Azure, because it requires, customer router configuration, last mile provider configuration. Therefore, I thought I would share my experience with the rest of the world. This blog post is around the experience, a bit of a glimpse, to what happens when you enable Azure Global Reach. A bit of background on the environment that I setup is below. I have two on-premises data centers, one in Seattle, one in Washington DC. They are connected to their local Azure regions respectively, in West US2 and East US. I have also cross connected the two ExpressRoute together so that one on-premises datacenter can talk to each other Azure datacenter using the ExpressRoute. This is allowed with a standard ExpressRoute between the same geopolitical region. However, this setup doesn’t allow me to talk from one on-premises datacenter to the other on-premises datacenter. For this to happen I would need Azure Global Reach enabled. Let do some testing to verify this behavior. As you can see in the above ping tests, from a test machines in Washington DC, I can reach to East US and West US2 but not the Seattle on-premises VM.My ExpressRoute Connections currently looks as below. There are two connections on each ExpressRoute, One to each Azure Datacenter region. If I check my routes that appear in one of the ExpressRoutes, It will look like below. Now let’s go ahead and enable Global Reach in my two ExpressRoutes. This is something you cannot do in the Azure Portal as of today. You must use PowerShell to enable Global Reach. The instructions to enable Global reach can be found in this article. In my case I ran the following PowerShell Commands. It creates following orange connection. 1234567891011121314151617$ER1RG &#x3D; &quot;Company17&quot;$ER1 &#x3D; &quot;Company17-er&quot;$ER2RG &#x3D; &quot;AComp17&quot;$ER2 &#x3D; &quot;AComp17-er&quot;$ckt_1 &#x3D; Get-AzExpressRouteCircuit -Name $ER1 -ResourceGroupName $ER1RG$ckt_2 &#x3D; Get-AzExpressRouteCircuit -Name $ER2 -ResourceGroupName $ER2RG $PeeringAddressPrefix1 &#x3D; &#39;10.200.0.0&#x2F;29&#39;Add-AzExpressRouteCircuitConnectionConfig -Name &#39;GlobalReach-EastUS&#39; -ExpressRouteCircuit $ckt_1 -PeerExpressRouteCircuitPeering $ckt_2.Peerings[0].Id -AddressPrefix $PeeringAddressPrefix1Set-AzExpressRouteCircuit -ExpressRouteCircuit $ckt_1 The Peering Address Prefix is a /29 ip range that does not overlap in your existing environment. Once the PowerShell commands are successfully executed, you will see this additional property on the ExpressRoute. It will appear only on the ExpressRoute that you execute the command. It will not appear on the other ExpressRoute. However, routing will work both ways. If I go back and do my connectivity tests, I can see it is working as below. If I look at the advertised routes in an ExpressRoute now, you will see the following. You will see three additional entries with the peering prefix IP address you provided. The first one is the path to the Washington DC Datacenter from the Seattle datacenter. Because of this path, it adds two more additional paths to the East US Azure DC and West US 2 DC. Those are added as well.You will not see any changes on the ER connections in the portal. They remain the same. This is how technically Global reach works. I add additional routes to your ExpressRoute which advertises the other data center IP ranges in your local ExpressRoute.","categories":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/categories/networking/"}],"tags":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/tags/networking/"},{"name":"ExpressRoute","slug":"expressroute","permalink":"https://sameeraman.github.io/tags/expressroute/"},{"name":"GlobalReach","slug":"globalreach","permalink":"https://sameeraman.github.io/tags/globalreach/"}]},{"title":"Under the Hood of this Blog","slug":"underthehood","date":"2020-10-18T09:34:48.000Z","updated":"2022-01-29T06:18:40.812Z","comments":true,"path":"2020/10/18/underthehood/","link":"","permalink":"https://sameeraman.github.io/2020/10/18/underthehood/","excerpt":"I recently decided to shift my gears with static pages for my blog. I have been using wordpress for all my blog publication so far. You can find my old blog here. With the trend of static pages, I thought of re-inventing the backend of my blog. And here we are, you are reading my new blog based on static pages which is hosted on Github pages. In this blog post, I am going to show you what happens under the hood when I publish a new article.","text":"I recently decided to shift my gears with static pages for my blog. I have been using wordpress for all my blog publication so far. You can find my old blog here. With the trend of static pages, I thought of re-inventing the backend of my blog. And here we are, you are reading my new blog based on static pages which is hosted on Github pages. In this blog post, I am going to show you what happens under the hood when I publish a new article. I was aware that the static pages had some limits around support for comments, analytics etc. But things have progressed a quiet a bit recently. There are smart ways to achieve these and tools available to make our life simple. Therefore, blogs are moving to static pages in the developer community these days. The idea of the static pages-based blog is to keep the raw data of the blog some where and use a tool to generate the static pages for the blog. The raw data for the blog could be markdown files, JSON documents or any other meta data storing mechanism. There is plethora of tools available to convert the raw blog data to static pages. Hugo , Gatsby and Jekyllrb are a few to name. Once you have generated the static pages, you can publish the static pages in a several hosting places. A few of them are Github pages, Azure Storage Web, and AWS S3 bucket. As you can see there are plenty of options to choose from for each component. Each one of those, has its own strengths and weaknesses. If you are thinking of using static pages for your blog, best is to try a few of those and find the one that works for you. Now, let me explain you how this blog works. In very high-level, the following diagram depicts the process and what happens under the hood when I publish a new post. Firstly, I use Hexo as the static page generator. Hexo has specialized features for hosting blogs and it supports Markdown files as source. This tool simplified my comment system, analytics and RSS feeds etc. Then I built others around it. I use a Github private repo for storing my source blog metadata. I edit and author locally, then push and merge the changes to private Github repo. Then I have configured a Github action to trigger when there is a new commit to the blog source repo. It will use a build agent, install Hexo, NodeJS and other necessary dependencies, build the static pages and push them to the public blog repo. The public blog repo is configured with Github pages. Once the static pages are published, the pages will be available through the Github pages to the public internet. A new post release process is fully automated using Github actions as you can see. That is the under the hood story which generated this page you are reading today. There are themes available for Hexo which can be used to change the look and feel. I have used pure theme and customized it according how I want to look it like. I hope this post is informative for you. Hope this will convince you to start or migrate your blog over to static pages.","categories":[{"name":"DevOps","slug":"devops","permalink":"https://sameeraman.github.io/categories/devops/"}],"tags":[{"name":"Static Pages","slug":"static-pages","permalink":"https://sameeraman.github.io/tags/static-pages/"},{"name":"Github","slug":"github","permalink":"https://sameeraman.github.io/tags/github/"},{"name":"DevOps","slug":"devops","permalink":"https://sameeraman.github.io/tags/devops/"},{"name":"Hexo","slug":"hexo","permalink":"https://sameeraman.github.io/tags/hexo/"},{"name":"Github Actions","slug":"github-actions","permalink":"https://sameeraman.github.io/tags/github-actions/"}]},{"title":"Connect Ubiquiti USG to Azure VWAN Gateway using BGP","slug":"usg2vwanbgp","date":"2020-10-05T09:34:48.000Z","updated":"2022-01-29T06:18:40.812Z","comments":true,"path":"2020/10/05/usg2vwanbgp/","link":"","permalink":"https://sameeraman.github.io/2020/10/05/usg2vwanbgp/","excerpt":"In this blog post, I’m going to be sharing my knowledge that I gathered during a lab setup. Last weekend, I was playing with Ubiquiti USG BGP features and was wondering If I could establish BGP peering with my Azure VPN Gateway. This way, it could dynamically exchange routes between my home network and Azure. I typically have a hybrid networking configured between my home network and Azure. My Azure network is very dynamic, I create new VNETs and delete VNETs very often. Managing static routes in my home router and IP Sec tunnels in each setup has been very cumbersome. Therefore, my curious mind was always looking for a smarter way to do this. If you are in this same boat, join with me.","text":"In this blog post, I’m going to be sharing my knowledge that I gathered during a lab setup. Last weekend, I was playing with Ubiquiti USG BGP features and was wondering If I could establish BGP peering with my Azure VPN Gateway. This way, it could dynamically exchange routes between my home network and Azure. I typically have a hybrid networking configured between my home network and Azure. My Azure network is very dynamic, I create new VNETs and delete VNETs very often. Managing static routes in my home router and IP Sec tunnels in each setup has been very cumbersome. Therefore, my curious mind was always looking for a smarter way to do this. If you are in this same boat, join with me. In summary, BGP peering’s can be established between the Ubiquity USG and the Azure Gateway enabled with BGP. In this blog post, I’m focusing more on the USG configuration and assume that you can setup the rest of the environment by yourself. To explain how this can be setup, I’m going to use my lab as an example. Following is a diagram of my setup. Following are the key properties of the setup that you need to be aware. Home NetworkIP Addressing used in the Home network – 10.1.1.0/24Static Public IP on the USG – 113.76.252.224BGP Peering IP on the USG – 10.1.1.1 Azure Network – VWANVPN Gateway Public IP – 21.52.125.78Azure Gateway Peering IP – 10.0.1.14VWAN Hub IP Address space – 10.0.1.0/24VNET IP Address Space – 10.10.0.0/16 Note that in Azure I have used Azure VWAN for hub and spoke topology. To learn more about Azure VWAN click here. Azure VWAN Hub can have VPN Gateways. I assume that you have setup the Azure Networking piece beforehand, and I’m not going to be covering that piece in this article. You can refer to this article if you need some guidance on the VWAN hub and the VPN gateway setup. By default, it creates two VPN gateway instances. See below screenshot which displays the properties of the two gateways. In this case I have used only one gateway instance as my home network has only one gateway. Now you have all the details required for the VPN to setup. Let’s look at how to configure the USG. Unfortunately, USG configuration can’t be done via the GUI. You will need to use the advance configuration file config.gateway.json. For more details about the advanced configuration file visit this documentation.This file is in the cloud key and the location is explained in the documentation. In my case it is - /srv/unifi/data/sites/. If you are editing this for the first time, you will need to create the file. This needs to be a valid JSON file, therefore, be careful and always validate when editing this file.Add the following configuration to the file. Replace you public and local IP in here with your respective IPs. Save the configuration and do a force provision from the Cloud Key. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162&#123; &quot;system&quot;: &#123; &quot;static-host-mapping&quot;: &#123; &quot;host-name&quot;: &#123; &quot;test1.snmnest.local&quot;: &#123; &quot;alias&quot;: [ &quot;lab&quot; ], &quot;inet&quot;: [ &quot;10.1.1.13&quot; ] &#125; &#125; &#125; &#125;, &quot;interfaces&quot;: &#123; &quot;vti&quot;: &#123; &quot;vti0&quot;: &#123; &quot;mtu&quot;: &quot;1436&quot; &#125; &#125; &#125;, &quot;firewall&quot;: &#123; &quot;options&quot;: &#123; &quot;mss-clamp&quot;: &#123; &quot;interface-type&quot;: [ &quot;pppoe&quot;, &quot;pptp&quot;, &quot;vti&quot; ], &quot;mss&quot;: &quot;1350&quot; &#125; &#125; &#125;, &quot;protocols&quot;: &#123; &quot;bgp&quot;: &#123; &quot;65510&quot;: &#123; &quot;neighbor&quot;: &#123; &quot;10.0.1.14&quot;: &#123; &quot;ebgp-multihop&quot;: &quot;4&quot;, &quot;prefix-list&quot;: &#123; &quot;export&quot;: &quot;BGP&quot;, &quot;import&quot;: &quot;BGP&quot; &#125;, &quot;remote-as&quot;: &quot;65515&quot;, &quot;soft-reconfiguration&quot;: &#123; &quot;inbound&quot;: &quot;&#39;&#39;&quot; &#125;, &quot;update-source&quot;: &quot;10.1.1.1&quot; &#125; &#125;, &quot;network&quot;: &#123; &quot;10.1.1.0&#x2F;24&quot;: &quot;&#39;&#39;&quot; &#125;, &quot;timers&quot;: &#123; &quot;holdtime&quot;: &quot;180&quot;, &quot;keepalive&quot;: &quot;60&quot; &#125; &#125; &#125;, &quot;static&quot;: &#123; &quot;interface-route&quot;: &#123; &quot;10.0.1.14&#x2F;32&quot;: &#123; &quot;next-hop-interface&quot;: &#123; &quot;vti0&quot;: &quot;&#39;&#39;&quot; &#125; &#125; &#125; &#125; &#125;, &quot;policy&quot;: &#123; &quot;prefix-list&quot;: &#123; &quot;BGP&quot;: &#123; &quot;rule&quot;: &#123; &quot;10&quot;: &#123; &quot;action&quot;: &quot;deny&quot;, &quot;description&quot;: &quot;deny-localgw&quot;, &quot;prefix&quot;: &quot;113.76.252.224&#x2F;32&quot; &#125;, &quot;100&quot;: &#123; &quot;action&quot;: &quot;permit&quot;, &quot;description&quot;: &quot;permit-localsubnet&quot;, &quot;prefix&quot;: &quot;10.1.1.0&#x2F;24&quot; &#125;, &quot;110&quot;: &#123; &quot;action&quot;: &quot;permit&quot;, &quot;description&quot;: &quot;permit-remotesubnet&quot;, &quot;ge&quot;: &quot;16&quot;, &quot;prefix&quot;: &quot;10.0.0.0&#x2F;8&quot; &#125;, &quot;20&quot;: &#123; &quot;action&quot;: &quot;deny&quot;, &quot;description&quot;: &quot;deny-remotegw&quot;, &quot;prefix&quot;: &quot;21.52.125.78&#x2F;32&quot; &#125;, &quot;30&quot;: &#123; &quot;action&quot;: &quot;deny&quot;, &quot;description&quot;: &quot;deny-localpeer&quot;, &quot;prefix&quot;: &quot;10.1.1.1&#x2F;32&quot; &#125;, &quot;40&quot;: &#123; &quot;action&quot;: &quot;deny&quot;, &quot;description&quot;: &quot;deny-remotepeer&quot;, &quot;prefix&quot;: &quot;10.0.1.14&#x2F;32&quot; &#125; &#125; &#125; &#125; &#125;, &quot;vpn&quot;: &#123; &quot;ipsec&quot;: &#123; &quot;auto-firewall-nat-exclude&quot;: &quot;enable&quot;, &quot;esp-group&quot;: &#123; &quot;VWAN01&quot;: &#123; &quot;compression&quot;: &quot;disable&quot;, &quot;lifetime&quot;: &quot;27000&quot;, &quot;mode&quot;: &quot;tunnel&quot;, &quot;pfs&quot;: &quot;disable&quot;, &quot;proposal&quot;: &#123; &quot;1&quot;: &#123; &quot;encryption&quot;: &quot;aes256&quot;, &quot;hash&quot;: &quot;sha1&quot; &#125; &#125; &#125; &#125;, &quot;ike-group&quot;: &#123; &quot;VWAN01&quot;: &#123; &quot;ikev2-reauth&quot;: &quot;no&quot;, &quot;key-exchange&quot;: &quot;ikev2&quot;, &quot;lifetime&quot;: &quot;28800&quot;, &quot;proposal&quot;: &#123; &quot;1&quot;: &#123; &quot;dh-group&quot;: &quot;2&quot;, &quot;encryption&quot;: &quot;aes256&quot;, &quot;hash&quot;: &quot;sha1&quot; &#125; &#125; &#125; &#125;, &quot;site-to-site&quot;: &#123; &quot;peer&quot;: &#123; &quot;21.52.125.78&quot;: &#123; &quot;authentication&quot;: &#123; &quot;mode&quot;: &quot;pre-shared-secret&quot;, &quot;pre-shared-secret&quot;: &quot;mykeyhereplease&quot; &#125;, &quot;connection-type&quot;: &quot;respond&quot;, &quot;description&quot;: &quot;ipsec&quot;, &quot;ike-group&quot;: &quot;VWAN01&quot;, &quot;ikev2-reauth&quot;: &quot;inherit&quot;, &quot;local-address&quot;: &quot;113.76.252.224&quot;, &quot;vti&quot;: &#123; &quot;bind&quot;: &quot;vti0&quot;, &quot;esp-group&quot;: &quot;VWAN01&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; Once the configuration is pushed. Restart the USG. Once it’s successfully restarted, ssh into the USG. Then check the BGP status using the following commands. It should show the results as below. 1show ip bgp summary 1show ip bgp neighbor 1show ip bgp neighbors 10.0.1.14 advertised-routes This command shows the routes advertised to the remote peer 1show ip bgp neighbors 10.0.1.14 received-routes This command shows the routes recieved from the remote peer 1show ip bgp 1show ip route","categories":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/categories/networking/"}],"tags":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/tags/networking/"},{"name":"VWAN","slug":"vwan","permalink":"https://sameeraman.github.io/tags/vwan/"},{"name":"Ubiquiti","slug":"ubiquiti","permalink":"https://sameeraman.github.io/tags/ubiquiti/"}]},{"title":"Web App Private Endpoints vs Service Endpoints vs App Service Environments","slug":"internalwebapphostingoptions","date":"2020-07-09T10:34:48.000Z","updated":"2022-01-29T06:18:40.808Z","comments":true,"path":"2020/07/09/internalwebapphostingoptions/","link":"","permalink":"https://sameeraman.github.io/2020/07/09/internalwebapphostingoptions/","excerpt":"Recently, I got an opportunity to compare the Private Endpoint for Web Apps, Service Endpoints for Web Apps and App Service Environments. I thought it would be good to write a blog post on the observations and the comparison I learned. Limiting the attack vector for the web applications and exposing the application only to the customer private networks is a very common requirement when you talk to customers. Therefore, this blog is about a comparison between the most commonly used hosting methods for internal-facing web applications in Azure.","text":"Recently, I got an opportunity to compare the Private Endpoint for Web Apps, Service Endpoints for Web Apps and App Service Environments. I thought it would be good to write a blog post on the observations and the comparison I learned. Limiting the attack vector for the web applications and exposing the application only to the customer private networks is a very common requirement when you talk to customers. Therefore, this blog is about a comparison between the most commonly used hosting methods for internal-facing web applications in Azure. Private Endpoints for Web AppsWith the announcement of the Private Endpoints for Web Apps, it opens up a new architecture for making Azure Web Apps available to the Azure Networks. This method creates a Private IP address in your virtual network dedicated for the Web App instance you choose. This private IP can be used as a secure entry point for the Web Application. This feature combined with external access blocked to the web app makes the application available only to the local network. The traffic originating from an end-user traverse the virtual network to the private endpoint for the web app. Then it goes through the Microsoft Backbone to the Web App. It never goes to the Public Internet. Hence, this is a much secure way to have connectivity to internal-facing web applications. The above diagram shows the high-level architecture for the private endpoint for web apps. One of the key features of this architecture is that this model works with the hybrid networking configuration as well. This means, your on-premises users can route to your web application using the private endpoint in the virtual network. This also provides additional security features. Private endpoint makes sure that only the allowed web application instance can be accessed by the private endpoint. It can not access any other service through it. Therefore, it protects you from any possible data exfiltration from your network. Note that this feature is still in public preview as of 10/07/2020. Service Endpoints for Web AppsService Endpoints for Web Apps provide secure connectivity to Azure Web apps over an optmized route over the Microsoft backbone. When compared with the Private Endpoints, Service Endpoints does not provide a private IP in your network. Instead, it will add special routes to your VNet, so that the Web App traffic will route via the Microsoft backbone to the web app. Therefore, it will not leave the Microsoft network when reaching the web app. You can also combine Service Endpoints and restrict public access to only allow traffic from the virtual networks selected. The diagram above shows the high level architecture for the Service Endpoints for web apps. One of the key functional differences in service endpoint, when compared to private endpoints, is that this provides private access to the full service in the Azure Region whereas in Private link it is only to that instance. Service endpoints are simple to configure and easier option when compared to the other two. App Service EnvironmentsAzure App Service environments provide a fully isolated and dedicated environment in a customer network to run web apps. This provisions a dedicated instance of the app hosting plan in your network as opposed to the multi-tenanted offerings in the other two option. This allows customers to fully control network traffic that goes in and out of the web app. The above high-level architecture diagram shows how the App service environment sits in the network. Providing internal access to the application is easy since it sits within the virtual network. If you have hybrid connectivity via an ExpressRoute or a Site-to-Site VPN, then it can route to the web application without any special configuration. SummaryIn this post, we looked at different architecture options for hosting a Web App which is exposed to the internal network only. See the below table, as a summary of features, functionalities and complexities discussed above. Private Endpoints Service Endpoints App Service Environments Provide Access to Web Apps over Private IP Address Provide Access to Web Apps over optimized backbone routing Web Apps are provisioned within the customer network Access is restricted per Web App Instance Access is restricted per Web app Service Access restriction up to the customer as the service is in the customer network Complexity: Planning and initial configuration required Complexity: Easy to setup Complexity: Planning and Initial configuration required In-built data exfiltration protection Traffic will need to be passed through an NVA/Firewall for exfiltration protection N/A as the application is entirely in the customer network Cost compared to the other two options: moderate Cost compared to the other two options: Can be low and depends on the application Cost compared to the other two options: Higher running cost Multi-tenant Hosting plan Multi-tenant Hosting plan Dedicated hosting plan","categories":[],"tags":[{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/tags/networking/"},{"name":"Apps","slug":"apps","permalink":"https://sameeraman.github.io/tags/apps/"}]},{"title":"Docker Cheat Sheet","slug":"dockercheatsheet","date":"2020-07-04T09:34:48.000Z","updated":"2022-01-29T06:18:40.804Z","comments":true,"path":"2020/07/04/dockercheatsheet/","link":"","permalink":"https://sameeraman.github.io/2020/07/04/dockercheatsheet/","excerpt":"I recently went through my second study phase around docker containers recently. Docker is a huge topic these days and widly used in the application containerization. I use Docker at home and at work in many projects. I thought it would be useful to create a Docker Cheat Sheet for my reference and share that among the community and that lead to this post. In this post, I’m going to list the most common commands used when playing with Docker containers.","text":"I recently went through my second study phase around docker containers recently. Docker is a huge topic these days and widly used in the application containerization. I use Docker at home and at work in many projects. I thought it would be useful to create a Docker Cheat Sheet for my reference and share that among the community and that lead to this post. In this post, I’m going to list the most common commands used when playing with Docker containers. General Listing Commands List all docker containers 1docker ps -a List running docker containers 1docker ps -a List docker images in the local store 1docker images Download and store an image from the internet 1docker pull ubuntu Running Docker Containers Run an instance of a container image 1docker run ubuntu Run an instance of a container image in detached mode (run in background) 1docker run -d nginx Run an instance of a Ubuntu container images with terminal and input attached. 1docker run -it ubuntu bash Stopping and Removing Docker Containers Stop a running container instance 1docker stop ubuntu Remove a terminated container (Container Id = b5a12307c030) 1docker rm b5a12307c030 Remove a container image from the local store 1docker rmi ubuntu Container interaction Run a container instance interactively 1docker run -it ubuntu bash Remote into a running container instance (Container id = b5a12307c030 ) 1docker exec -it b5a12307c030 bash Execute a command in a running container Eg: ps -eaf 1docker exec b5a12307c030 ps -eaf Map a Port to a Container Map a host port to a new container instance (Host Port = 5000, Container Port = 80, Container image = nginx)1docker run -p 80:5000 nginx -d Mount a Volume Mount a host volume to a new container (Host Volume Path = /opt/mydata, Container Mount Path = /var/lib/mysql)1docker run -v &#x2F;opt&#x2F;mydata:&#x2F;var&#x2F;lib&#x2F;mysql mysql Inspect Container Logs Inspect container logs of Container ID = b5a12307c0301docker logs b5a12307c030 Other Handy Commands Remove all excited Containers 1docker rm -f $(docker ps -q --filter &quot;status&#x3D;exited&quot;) Monitor Docker disk consumption 1docker system df","categories":[{"name":"Containers","slug":"containers","permalink":"https://sameeraman.github.io/categories/containers/"}],"tags":[{"name":"Containers","slug":"containers","permalink":"https://sameeraman.github.io/tags/containers/"},{"name":"Docker","slug":"docker","permalink":"https://sameeraman.github.io/tags/docker/"}]}],"categories":[{"name":"AKS","slug":"aks","permalink":"https://sameeraman.github.io/categories/aks/"},{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/categories/networking/"},{"name":"Governance","slug":"governance","permalink":"https://sameeraman.github.io/categories/governance/"},{"name":"DevOps","slug":"devops","permalink":"https://sameeraman.github.io/categories/devops/"},{"name":"Containers","slug":"containers","permalink":"https://sameeraman.github.io/categories/containers/"}],"tags":[{"name":"AKS","slug":"aks","permalink":"https://sameeraman.github.io/tags/aks/"},{"name":"Kuberenetes","slug":"kuberenetes","permalink":"https://sameeraman.github.io/tags/kuberenetes/"},{"name":"Storage","slug":"storage","permalink":"https://sameeraman.github.io/tags/storage/"},{"name":"Files","slug":"files","permalink":"https://sameeraman.github.io/tags/files/"},{"name":"Networking","slug":"networking","permalink":"https://sameeraman.github.io/tags/networking/"},{"name":"ExpressRoute","slug":"expressroute","permalink":"https://sameeraman.github.io/tags/expressroute/"},{"name":"PrivateLink","slug":"privatelink","permalink":"https://sameeraman.github.io/tags/privatelink/"},{"name":"AzCopy","slug":"azcopy","permalink":"https://sameeraman.github.io/tags/azcopy/"},{"name":"Governance","slug":"governance","permalink":"https://sameeraman.github.io/tags/governance/"},{"name":"VWAN","slug":"vwan","permalink":"https://sameeraman.github.io/tags/vwan/"},{"name":"Bicep","slug":"bicep","permalink":"https://sameeraman.github.io/tags/bicep/"},{"name":"GlobalReach","slug":"globalreach","permalink":"https://sameeraman.github.io/tags/globalreach/"},{"name":"Static Pages","slug":"static-pages","permalink":"https://sameeraman.github.io/tags/static-pages/"},{"name":"Github","slug":"github","permalink":"https://sameeraman.github.io/tags/github/"},{"name":"DevOps","slug":"devops","permalink":"https://sameeraman.github.io/tags/devops/"},{"name":"Hexo","slug":"hexo","permalink":"https://sameeraman.github.io/tags/hexo/"},{"name":"Github Actions","slug":"github-actions","permalink":"https://sameeraman.github.io/tags/github-actions/"},{"name":"Ubiquiti","slug":"ubiquiti","permalink":"https://sameeraman.github.io/tags/ubiquiti/"},{"name":"Apps","slug":"apps","permalink":"https://sameeraman.github.io/tags/apps/"},{"name":"Containers","slug":"containers","permalink":"https://sameeraman.github.io/tags/containers/"},{"name":"Docker","slug":"docker","permalink":"https://sameeraman.github.io/tags/docker/"}]}